{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "** GANs for generating synthetic datasets**"
      ],
      "metadata": {
        "id": "XBJUwfZnGQx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim),\n",
        "            nn.Tanh()  # Output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()  # Probability output\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Hyperparameters\n",
        "noise_dim = 100\n",
        "data_dim = 784  # e.g., flattened 28x28 images\n",
        "lr = 0.0002\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "\n",
        "# Initialize models and optimizers\n",
        "G = Generator(noise_dim, data_dim)\n",
        "D = Discriminator(data_dim)\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Training loop (simplified)\n",
        "for epoch in range(epochs):\n",
        "    # Assume real_data is a batch from the dataset\n",
        "    real_data = torch.randn(batch_size, data_dim)  # Placeholder\n",
        "    real_labels = torch.ones(batch_size, 1)\n",
        "    fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "    # Train Discriminator\n",
        "    D_optimizer.zero_grad()\n",
        "    real_output = D(real_data)\n",
        "    d_loss_real = criterion(real_output, real_labels)\n",
        "\n",
        "    noise = torch.randn(batch_size, noise_dim)\n",
        "    fake_data = G(noise)\n",
        "    fake_output = D(fake_data.detach())\n",
        "    d_loss_fake = criterion(fake_output, fake_labels)\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "    # Train Generator\n",
        "    G_optimizer.zero_grad()\n",
        "    fake_output = D(fake_data)\n",
        "    g_loss = criterion(fake_output, real_labels)  # Fool the Discriminator\n",
        "    g_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
        "\n",
        "# Generate synthetic data\n",
        "noise = torch.randn(10, noise_dim)\n",
        "synthetic_data = G(noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w3hqTZ9GgPY",
        "outputId": "0cbd7592-6fab-44d4-d0c3-35f329969d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, D Loss: 1.3849598169326782, G Loss: 0.6848213076591492\n",
            "Epoch 1, D Loss: 1.3269908428192139, G Loss: 0.6908121109008789\n",
            "Epoch 2, D Loss: 1.3303923606872559, G Loss: 0.6952741742134094\n",
            "Epoch 3, D Loss: 1.3170936107635498, G Loss: 0.7015909552574158\n",
            "Epoch 4, D Loss: 1.3430728912353516, G Loss: 0.7062100172042847\n",
            "Epoch 5, D Loss: 1.3042454719543457, G Loss: 0.7106066942214966\n",
            "Epoch 6, D Loss: 1.3027377128601074, G Loss: 0.7131630182266235\n",
            "Epoch 7, D Loss: 1.3012773990631104, G Loss: 0.7199525833129883\n",
            "Epoch 8, D Loss: 1.2798326015472412, G Loss: 0.7231944799423218\n",
            "Epoch 9, D Loss: 1.253566026687622, G Loss: 0.7298039197921753\n",
            "Epoch 10, D Loss: 1.2780611515045166, G Loss: 0.7312931418418884\n",
            "Epoch 11, D Loss: 1.2739906311035156, G Loss: 0.7366844415664673\n",
            "Epoch 12, D Loss: 1.2737189531326294, G Loss: 0.7363312244415283\n",
            "Epoch 13, D Loss: 1.2524734735488892, G Loss: 0.7412818074226379\n",
            "Epoch 14, D Loss: 1.2437070608139038, G Loss: 0.7450071573257446\n",
            "Epoch 15, D Loss: 1.2442865371704102, G Loss: 0.7516727447509766\n",
            "Epoch 16, D Loss: 1.2213197946548462, G Loss: 0.7518734335899353\n",
            "Epoch 17, D Loss: 1.227677822113037, G Loss: 0.7549710273742676\n",
            "Epoch 18, D Loss: 1.2458233833312988, G Loss: 0.7581861019134521\n",
            "Epoch 19, D Loss: 1.223839521408081, G Loss: 0.7580872774124146\n",
            "Epoch 20, D Loss: 1.2269290685653687, G Loss: 0.7597646713256836\n",
            "Epoch 21, D Loss: 1.1995291709899902, G Loss: 0.7605565190315247\n",
            "Epoch 22, D Loss: 1.2183728218078613, G Loss: 0.7628925442695618\n",
            "Epoch 23, D Loss: 1.2276554107666016, G Loss: 0.7635701298713684\n",
            "Epoch 24, D Loss: 1.2229385375976562, G Loss: 0.7656447291374207\n",
            "Epoch 25, D Loss: 1.1902711391448975, G Loss: 0.7630988955497742\n",
            "Epoch 26, D Loss: 1.2036550045013428, G Loss: 0.769875705242157\n",
            "Epoch 27, D Loss: 1.156587839126587, G Loss: 0.7650226354598999\n",
            "Epoch 28, D Loss: 1.2203090190887451, G Loss: 0.7626407146453857\n",
            "Epoch 29, D Loss: 1.2091374397277832, G Loss: 0.7633174657821655\n",
            "Epoch 30, D Loss: 1.2433078289031982, G Loss: 0.7607718706130981\n",
            "Epoch 31, D Loss: 1.16867995262146, G Loss: 0.7634086012840271\n",
            "Epoch 32, D Loss: 1.2036136388778687, G Loss: 0.7619410157203674\n",
            "Epoch 33, D Loss: 1.210263729095459, G Loss: 0.7547972202301025\n",
            "Epoch 34, D Loss: 1.2038004398345947, G Loss: 0.7534422874450684\n",
            "Epoch 35, D Loss: 1.1928329467773438, G Loss: 0.7560319304466248\n",
            "Epoch 36, D Loss: 1.2304396629333496, G Loss: 0.7519780397415161\n",
            "Epoch 37, D Loss: 1.1872228384017944, G Loss: 0.7462092638015747\n",
            "Epoch 38, D Loss: 1.2014257907867432, G Loss: 0.7476297616958618\n",
            "Epoch 39, D Loss: 1.2169569730758667, G Loss: 0.7446746826171875\n",
            "Epoch 40, D Loss: 1.1900439262390137, G Loss: 0.743742048740387\n",
            "Epoch 41, D Loss: 1.2062575817108154, G Loss: 0.730175256729126\n",
            "Epoch 42, D Loss: 1.2123064994812012, G Loss: 0.7330792546272278\n",
            "Epoch 43, D Loss: 1.1964216232299805, G Loss: 0.7311782240867615\n",
            "Epoch 44, D Loss: 1.1920766830444336, G Loss: 0.7319439649581909\n",
            "Epoch 45, D Loss: 1.183919906616211, G Loss: 0.7274406552314758\n",
            "Epoch 46, D Loss: 1.2127127647399902, G Loss: 0.72071772813797\n",
            "Epoch 47, D Loss: 1.2057534456253052, G Loss: 0.715355396270752\n",
            "Epoch 48, D Loss: 1.1906019449234009, G Loss: 0.7123338580131531\n",
            "Epoch 49, D Loss: 1.2121950387954712, G Loss: 0.7117758989334106\n",
            "Epoch 50, D Loss: 1.2096028327941895, G Loss: 0.7081028819084167\n",
            "Epoch 51, D Loss: 1.2034318447113037, G Loss: 0.7093838453292847\n",
            "Epoch 52, D Loss: 1.239801049232483, G Loss: 0.699629008769989\n",
            "Epoch 53, D Loss: 1.2261853218078613, G Loss: 0.7034964561462402\n",
            "Epoch 54, D Loss: 1.1944957971572876, G Loss: 0.7067916989326477\n",
            "Epoch 55, D Loss: 1.2218785285949707, G Loss: 0.7012151479721069\n",
            "Epoch 56, D Loss: 1.2111953496932983, G Loss: 0.7000311613082886\n",
            "Epoch 57, D Loss: 1.2151657342910767, G Loss: 0.6962982416152954\n",
            "Epoch 58, D Loss: 1.2353450059890747, G Loss: 0.6911836266517639\n",
            "Epoch 59, D Loss: 1.2229478359222412, G Loss: 0.688615083694458\n",
            "Epoch 60, D Loss: 1.224109411239624, G Loss: 0.6930772066116333\n",
            "Epoch 61, D Loss: 1.224023699760437, G Loss: 0.6794761419296265\n",
            "Epoch 62, D Loss: 1.2002043724060059, G Loss: 0.6961605548858643\n",
            "Epoch 63, D Loss: 1.2299094200134277, G Loss: 0.6904058456420898\n",
            "Epoch 64, D Loss: 1.2330653667449951, G Loss: 0.6877856254577637\n",
            "Epoch 65, D Loss: 1.228355884552002, G Loss: 0.6936326026916504\n",
            "Epoch 66, D Loss: 1.2260916233062744, G Loss: 0.6877059936523438\n",
            "Epoch 67, D Loss: 1.2522637844085693, G Loss: 0.6926165223121643\n",
            "Epoch 68, D Loss: 1.2201101779937744, G Loss: 0.696500301361084\n",
            "Epoch 69, D Loss: 1.2087233066558838, G Loss: 0.6959725022315979\n",
            "Epoch 70, D Loss: 1.177236795425415, G Loss: 0.6963236331939697\n",
            "Epoch 71, D Loss: 1.1696468591690063, G Loss: 0.7027280926704407\n",
            "Epoch 72, D Loss: 1.1932897567749023, G Loss: 0.6971431970596313\n",
            "Epoch 73, D Loss: 1.2041552066802979, G Loss: 0.6951573491096497\n",
            "Epoch 74, D Loss: 1.1727585792541504, G Loss: 0.6945940256118774\n",
            "Epoch 75, D Loss: 1.167807936668396, G Loss: 0.6940963268280029\n",
            "Epoch 76, D Loss: 1.218736171722412, G Loss: 0.6921464204788208\n",
            "Epoch 77, D Loss: 1.1987075805664062, G Loss: 0.7022047638893127\n",
            "Epoch 78, D Loss: 1.1961877346038818, G Loss: 0.7082552909851074\n",
            "Epoch 79, D Loss: 1.1534390449523926, G Loss: 0.7072091102600098\n",
            "Epoch 80, D Loss: 1.1676411628723145, G Loss: 0.6926508545875549\n",
            "Epoch 81, D Loss: 1.1661219596862793, G Loss: 0.698062539100647\n",
            "Epoch 82, D Loss: 1.1779015064239502, G Loss: 0.7007919549942017\n",
            "Epoch 83, D Loss: 1.1414508819580078, G Loss: 0.711088240146637\n",
            "Epoch 84, D Loss: 1.2092933654785156, G Loss: 0.6964007616043091\n",
            "Epoch 85, D Loss: 1.1984844207763672, G Loss: 0.6993367671966553\n",
            "Epoch 86, D Loss: 1.1602383852005005, G Loss: 0.7061865925788879\n",
            "Epoch 87, D Loss: 1.1339740753173828, G Loss: 0.7102972269058228\n",
            "Epoch 88, D Loss: 1.1579546928405762, G Loss: 0.7120710611343384\n",
            "Epoch 89, D Loss: 1.1353764533996582, G Loss: 0.7125710248947144\n",
            "Epoch 90, D Loss: 1.1545459032058716, G Loss: 0.7056857943534851\n",
            "Epoch 91, D Loss: 1.1503854990005493, G Loss: 0.7100368738174438\n",
            "Epoch 92, D Loss: 1.1638180017471313, G Loss: 0.7037702798843384\n",
            "Epoch 93, D Loss: 1.1547671556472778, G Loss: 0.7115522623062134\n",
            "Epoch 94, D Loss: 1.1619560718536377, G Loss: 0.707360565662384\n",
            "Epoch 95, D Loss: 1.1349918842315674, G Loss: 0.7079079747200012\n",
            "Epoch 96, D Loss: 1.1233205795288086, G Loss: 0.696646511554718\n",
            "Epoch 97, D Loss: 1.145349144935608, G Loss: 0.7034335136413574\n",
            "Epoch 98, D Loss: 1.1402068138122559, G Loss: 0.7025095820426941\n",
            "Epoch 99, D Loss: 1.165795087814331, G Loss: 0.7013342976570129\n",
            "Epoch 100, D Loss: 1.1235076189041138, G Loss: 0.7087683081626892\n",
            "Epoch 101, D Loss: 1.1403510570526123, G Loss: 0.6956267356872559\n",
            "Epoch 102, D Loss: 1.133835792541504, G Loss: 0.7073947191238403\n",
            "Epoch 103, D Loss: 1.1412959098815918, G Loss: 0.703826367855072\n",
            "Epoch 104, D Loss: 1.1293764114379883, G Loss: 0.6953184604644775\n",
            "Epoch 105, D Loss: 1.1325486898422241, G Loss: 0.6976277232170105\n",
            "Epoch 106, D Loss: 1.1229952573776245, G Loss: 0.7038382291793823\n",
            "Epoch 107, D Loss: 1.1360647678375244, G Loss: 0.6982718706130981\n",
            "Epoch 108, D Loss: 1.1477479934692383, G Loss: 0.7033828496932983\n",
            "Epoch 109, D Loss: 1.1212102174758911, G Loss: 0.7091912031173706\n",
            "Epoch 110, D Loss: 1.1513222455978394, G Loss: 0.7048808336257935\n",
            "Epoch 111, D Loss: 1.1247413158416748, G Loss: 0.6969907283782959\n",
            "Epoch 112, D Loss: 1.1298599243164062, G Loss: 0.7059941291809082\n",
            "Epoch 113, D Loss: 1.1145546436309814, G Loss: 0.6933314800262451\n",
            "Epoch 114, D Loss: 1.1337553262710571, G Loss: 0.7025185227394104\n",
            "Epoch 115, D Loss: 1.143906593322754, G Loss: 0.7086591124534607\n",
            "Epoch 116, D Loss: 1.1185545921325684, G Loss: 0.6955466270446777\n",
            "Epoch 117, D Loss: 1.1100839376449585, G Loss: 0.701145350933075\n",
            "Epoch 118, D Loss: 1.1142605543136597, G Loss: 0.7128151655197144\n",
            "Epoch 119, D Loss: 1.0839707851409912, G Loss: 0.7128855586051941\n",
            "Epoch 120, D Loss: 1.1113009452819824, G Loss: 0.7086412310600281\n",
            "Epoch 121, D Loss: 1.0776454210281372, G Loss: 0.7191370129585266\n",
            "Epoch 122, D Loss: 1.057186245918274, G Loss: 0.7200267314910889\n",
            "Epoch 123, D Loss: 1.1143107414245605, G Loss: 0.7134000062942505\n",
            "Epoch 124, D Loss: 1.065948724746704, G Loss: 0.7215210199356079\n",
            "Epoch 125, D Loss: 1.0495085716247559, G Loss: 0.7237118482589722\n",
            "Epoch 126, D Loss: 1.0676971673965454, G Loss: 0.7266976833343506\n",
            "Epoch 127, D Loss: 1.0817204713821411, G Loss: 0.7368609309196472\n",
            "Epoch 128, D Loss: 1.0704976320266724, G Loss: 0.7461082339286804\n",
            "Epoch 129, D Loss: 1.081690788269043, G Loss: 0.7541701793670654\n",
            "Epoch 130, D Loss: 1.0554990768432617, G Loss: 0.7505396604537964\n",
            "Epoch 131, D Loss: 1.058002233505249, G Loss: 0.756235659122467\n",
            "Epoch 132, D Loss: 1.045227289199829, G Loss: 0.7610973715782166\n",
            "Epoch 133, D Loss: 1.05088210105896, G Loss: 0.7683303356170654\n",
            "Epoch 134, D Loss: 1.0511858463287354, G Loss: 0.7879565954208374\n",
            "Epoch 135, D Loss: 1.0172343254089355, G Loss: 0.7796556949615479\n",
            "Epoch 136, D Loss: 1.0103908777236938, G Loss: 0.7786968350410461\n",
            "Epoch 137, D Loss: 1.019616961479187, G Loss: 0.797382652759552\n",
            "Epoch 138, D Loss: 1.0143067836761475, G Loss: 0.8113134503364563\n",
            "Epoch 139, D Loss: 1.001524806022644, G Loss: 0.8095732927322388\n",
            "Epoch 140, D Loss: 0.9832744598388672, G Loss: 0.8155544996261597\n",
            "Epoch 141, D Loss: 0.9679736495018005, G Loss: 0.8375083208084106\n",
            "Epoch 142, D Loss: 1.0116347074508667, G Loss: 0.8240456581115723\n",
            "Epoch 143, D Loss: 0.9755860567092896, G Loss: 0.8382287621498108\n",
            "Epoch 144, D Loss: 0.989591658115387, G Loss: 0.847661018371582\n",
            "Epoch 145, D Loss: 1.0251898765563965, G Loss: 0.8220841288566589\n",
            "Epoch 146, D Loss: 0.9890627861022949, G Loss: 0.8352600932121277\n",
            "Epoch 147, D Loss: 1.014420509338379, G Loss: 0.8393366932868958\n",
            "Epoch 148, D Loss: 0.9930285215377808, G Loss: 0.8478171825408936\n",
            "Epoch 149, D Loss: 1.0286824703216553, G Loss: 0.8467702269554138\n",
            "Epoch 150, D Loss: 0.9807597398757935, G Loss: 0.8394960165023804\n",
            "Epoch 151, D Loss: 0.9964234232902527, G Loss: 0.8460038304328918\n",
            "Epoch 152, D Loss: 0.9745039939880371, G Loss: 0.863710343837738\n",
            "Epoch 153, D Loss: 1.0271884202957153, G Loss: 0.8428730368614197\n",
            "Epoch 154, D Loss: 1.003428339958191, G Loss: 0.8402942419052124\n",
            "Epoch 155, D Loss: 1.034327507019043, G Loss: 0.8318345546722412\n",
            "Epoch 156, D Loss: 1.0090566873550415, G Loss: 0.8491061329841614\n",
            "Epoch 157, D Loss: 1.0157488584518433, G Loss: 0.8415152430534363\n",
            "Epoch 158, D Loss: 1.0259499549865723, G Loss: 0.8205903768539429\n",
            "Epoch 159, D Loss: 1.011486291885376, G Loss: 0.844737708568573\n",
            "Epoch 160, D Loss: 0.9827085137367249, G Loss: 0.8476396799087524\n",
            "Epoch 161, D Loss: 1.0007060766220093, G Loss: 0.8552272915840149\n",
            "Epoch 162, D Loss: 1.009758710861206, G Loss: 0.8429009318351746\n",
            "Epoch 163, D Loss: 0.9770398736000061, G Loss: 0.8466281890869141\n",
            "Epoch 164, D Loss: 0.9954940676689148, G Loss: 0.864926815032959\n",
            "Epoch 165, D Loss: 0.9588955640792847, G Loss: 0.8871289491653442\n",
            "Epoch 166, D Loss: 0.9431164860725403, G Loss: 0.9015161991119385\n",
            "Epoch 167, D Loss: 1.0234862565994263, G Loss: 0.8958826065063477\n",
            "Epoch 168, D Loss: 0.9791556596755981, G Loss: 0.8982877135276794\n",
            "Epoch 169, D Loss: 0.9573743343353271, G Loss: 0.9234135150909424\n",
            "Epoch 170, D Loss: 1.027284026145935, G Loss: 0.9031898379325867\n",
            "Epoch 171, D Loss: 0.9816340208053589, G Loss: 0.9288065433502197\n",
            "Epoch 172, D Loss: 0.9702704548835754, G Loss: 0.9269545078277588\n",
            "Epoch 173, D Loss: 0.9456073641777039, G Loss: 0.9459644556045532\n",
            "Epoch 174, D Loss: 0.9844605326652527, G Loss: 0.9327220916748047\n",
            "Epoch 175, D Loss: 0.9464542865753174, G Loss: 0.9778624773025513\n",
            "Epoch 176, D Loss: 0.97252357006073, G Loss: 0.9271258115768433\n",
            "Epoch 177, D Loss: 0.9860333800315857, G Loss: 0.9453057646751404\n",
            "Epoch 178, D Loss: 0.9971771240234375, G Loss: 0.9565887451171875\n",
            "Epoch 179, D Loss: 0.9502792358398438, G Loss: 0.974638819694519\n",
            "Epoch 180, D Loss: 0.990851879119873, G Loss: 0.9682209491729736\n",
            "Epoch 181, D Loss: 1.0236380100250244, G Loss: 0.9383043050765991\n",
            "Epoch 182, D Loss: 0.99383544921875, G Loss: 0.998360276222229\n",
            "Epoch 183, D Loss: 1.021704077720642, G Loss: 0.9492254257202148\n",
            "Epoch 184, D Loss: 0.9894236326217651, G Loss: 0.9669892191886902\n",
            "Epoch 185, D Loss: 1.0036358833312988, G Loss: 0.9755366444587708\n",
            "Epoch 186, D Loss: 0.9702907204627991, G Loss: 0.9869917035102844\n",
            "Epoch 187, D Loss: 1.0104198455810547, G Loss: 0.9732227921485901\n",
            "Epoch 188, D Loss: 1.0138884782791138, G Loss: 0.9782540202140808\n",
            "Epoch 189, D Loss: 1.0406769514083862, G Loss: 1.0150797367095947\n",
            "Epoch 190, D Loss: 1.0055772066116333, G Loss: 0.9767669439315796\n",
            "Epoch 191, D Loss: 1.0217969417572021, G Loss: 0.992091953754425\n",
            "Epoch 192, D Loss: 1.0067180395126343, G Loss: 1.0025900602340698\n",
            "Epoch 193, D Loss: 0.9398053884506226, G Loss: 1.0256421566009521\n",
            "Epoch 194, D Loss: 1.0166749954223633, G Loss: 1.0126936435699463\n",
            "Epoch 195, D Loss: 1.0412224531173706, G Loss: 1.0116103887557983\n",
            "Epoch 196, D Loss: 0.9880719184875488, G Loss: 1.0532490015029907\n",
            "Epoch 197, D Loss: 0.991004228591919, G Loss: 1.0072702169418335\n",
            "Epoch 198, D Loss: 1.0220999717712402, G Loss: 1.0169737339019775\n",
            "Epoch 199, D Loss: 0.9912705421447754, G Loss: 1.0725762844085693\n"
          ]
        }
      ]
    }
  ]
}